{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3988be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0531 16:51:18.662310 139792472610624 builder.py:816] Found cached dataset json (/nvme/wangruohui/hfcache/datasets/allenai___json/allenai--c4-efc3d4f4606f44bd/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'timestamp', 'url'],\n",
      "    num_rows: 45576\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from mytool.data import get_data\n",
    "dataloader = get_data(\"c4\", \"validation\", first_n=128, tokenize=True, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from single import Transformer, ModelArgs\n",
    "import torch\n",
    "\n",
    "\n",
    "def init_model(device=\"cuda\", weight_path=\"full_fused.pth\"):\n",
    "    args = ModelArgs()\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    model = Transformer(args).to(device)\n",
    "    print(f\"create model \\t {next(iter(model.parameters())).device} {next(iter(model.parameters())).dtype}\")\n",
    "    state_dict = torch.load(weight_path, map_location=\"cuda\")\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0cdd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 3, max: 11, mean: 6.25, var: 9.687499999999998, count: 4\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Stat:\n",
    "    min: float = 0\n",
    "    max: float = 0\n",
    "    mean: float = 0\n",
    "    var: float = 0\n",
    "    count: int = 0\n",
    "\n",
    "    def update(self, value):\n",
    "        if self.count == 0:\n",
    "            self.min = value\n",
    "            self.max = value\n",
    "            self.mean = value\n",
    "            # self.M2 = 0\n",
    "            self.var = 0\n",
    "        else:\n",
    "            self.min = min(self.min, value)\n",
    "            self.max = max(self.max, value)\n",
    "            # old_mean = self.mean\n",
    "            self.var = (\n",
    "                self.var * self.count / (self.count + 1)\n",
    "                + (value - self.mean) ** 2 * self.count / (self.count + 1) ** 2\n",
    "            )\n",
    "            self.mean = (self.mean * self.count + value) / (self.count + 1)\n",
    "            # self.M2 += (value-self.mean)*(value-old_mean)\n",
    "            # self.var = self.M2 / (self.count + 1)\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"min: {self.min}, max: {self.max}, mean: {self.mean}, var: {self.var}, count: {self.count}\"\n",
    "\n",
    "\n",
    "vals = [3, 4, 7, 11]\n",
    "stat = Stat()\n",
    "for v in vals:\n",
    "    stat.update(v)\n",
    "\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7984c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: tensor([ 1., 32., 35.], dtype=torch.float64)\n",
      "max: tensor([ 6., 72., 57.], dtype=torch.float64)\n",
      "mean: tensor([ 3.7500, 44.5000, 50.0000], dtype=torch.float64)\n",
      "std: tensor([ 1.9203, 16.3936,  8.7750], dtype=torch.float64)\n",
      "count: 4\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Stat:\n",
    "    min: float = 0\n",
    "    max: float = 0\n",
    "    mean: float = 0\n",
    "    var: float = 0\n",
    "    count: int = 0\n",
    "\n",
    "    def update(self, value):\n",
    "        if self.count == 0:\n",
    "            self.min = value.clone().to(torch.float64)\n",
    "            self.max = value.clone().to(torch.float64)\n",
    "            self.mean = value.clone().to(torch.float64)\n",
    "            self.var = 0\n",
    "        else:\n",
    "            self.min = torch.minimum(self.min, value)\n",
    "            self.max = torch.maximum(self.max, value)\n",
    "            self.var = (\n",
    "                self.var * self.count / (self.count + 1)\n",
    "                + (value - self.mean) ** 2 * self.count / (self.count + 1) ** 2\n",
    "            )\n",
    "            self.mean = (self.mean * self.count + value) / (self.count + 1)\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return torch.sqrt(self.var)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"min: {self.min}\\nmax: {self.max}\\nmean: {self.mean}\\nstd: {self.std}\\ncount: {self.count}\"\n",
    "\n",
    "stat = Stat()    \n",
    "stat.update(torch.tensor([1,42,53]))\n",
    "stat.update(torch.tensor([3,32,55]))\n",
    "stat.update(torch.tensor([6,32,35]))\n",
    "stat.update(torch.tensor([5,72,57]))\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f18297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: tensor([ 1., 32., 35.], dtype=torch.float64)\n",
      "max: tensor([ 6., 72., 57.], dtype=torch.float64)\n",
      "mean: tensor([ 3.7500, 44.5000, 50.0000], dtype=torch.float64)\n",
      "std: tensor([ 1.9203, 16.3936,  8.7750], dtype=torch.float64)\n",
      "count: 4\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Stat:\n",
    "    min: float = 0\n",
    "    max: float = 0\n",
    "    mean: float = 0\n",
    "    M2: float = 0\n",
    "    count: int = 0\n",
    "\n",
    "    def update(self, value):\n",
    "        assert value.ndim == 2\n",
    "        local_count = value.shape[0]\n",
    "        local_min = value.amin(dim=0).to(torch.float64)\n",
    "        local_max = value.amax(dim=0).to(torch.float64)\n",
    "        local_mean = value.mean(dim=0).to(torch.float64)\n",
    "        local_M2 = value.var(dim=0, correction=0).to(torch.float64) * local_count\n",
    "\n",
    "        if self.count == 0:\n",
    "            self.min, self.max = local_min, local_max\n",
    "            self.mean, self.M2 = local_mean, local_M2\n",
    "        else:\n",
    "            self.min = torch.minimum(self.min, local_min)\n",
    "            self.max = torch.maximum(self.max, local_max)\n",
    "            delta = local_mean - self.mean\n",
    "            new_count = self.count + local_count\n",
    "            self.M2 = self.M2 + local_M2 + delta ** 2 * self.count * local_count / new_count\n",
    "            self.mean = self.mean + delta * local_count / new_count\n",
    "        \n",
    "        self.count += local_count\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return torch.sqrt(self.M2 / self.count)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"min: {self.min}\\nmax: {self.max}\\nmean: {self.mean}\\nstd: {self.std}\\ncount: {self.count}\"\n",
    "\n",
    "\n",
    "stat = Stat()\n",
    "stat.update(torch.tensor([[1., 42, 53],[3., 32, 55]]))\n",
    "stat.update(torch.tensor([[6., 32, 35]]))\n",
    "stat.update(torch.tensor([[5., 72, 57]]))\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e59a4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerChannelStatHook:\n",
    "    stat = {}\n",
    "\n",
    "    def __init__(self, name, which=\"output\") -> None:\n",
    "        name = str(name) + \"-\" + which\n",
    "        assert name not in self.__class__.stat\n",
    "        self.name = name\n",
    "        self.which = which\n",
    "        self.stat = Stat()\n",
    "        self.__class__.stat[name] = self.stat\n",
    "\n",
    "    def __call__(self, m, i, o):\n",
    "        if self.which == \"output\":\n",
    "            value = o\n",
    "        elif self.which == \"input\":\n",
    "            value = i\n",
    "\n",
    "        assert isinstance(value, torch.Tensor)\n",
    "        self.stat.update(value.squeeze())\n",
    "    \n",
    "    @classmethod\n",
    "    def clear(cls):\n",
    "        cls.stat = {}\n",
    "\n",
    "DEVICE = \"cuda:3\"\n",
    "model = init_model(device=DEVICE, weight_path=\"full_fused.pth\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if any(c in name for c in [\"_norm\", \"attention.\", \"feed_forward.\"]):\n",
    "        # print(name)\n",
    "        hook = PerChannelStatHook(name, \"output\")\n",
    "        module.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d7f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL 6.7389750480651855\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "total_loss = 0\n",
    "total_token = 0\n",
    "ns = 128\n",
    "\n",
    "for token in dataloader[:ns]:\n",
    "    if len(token) > 2048:\n",
    "        token = token[:2048]\n",
    "    token = torch.tensor([token]).to(DEVICE)\n",
    "    logits = model(token, 0)\n",
    "    # print(logits.shape)\n",
    "    shift_logits = logits[:, :-1, :]\n",
    "    shift_labels = token[:, 1:]\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "    )\n",
    "    # print(loss)\n",
    "    total_loss += loss.float()\n",
    "    total_token += shift_labels.size(-1)\n",
    "\n",
    "ppl = torch.exp(torch.sum(total_loss) / total_token)\n",
    "print(\"PPL\", ppl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "466b53a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['layers.0.attention.wq-output', 'layers.0.attention.wk-output', 'layers.0.attention.wv-output', 'layers.0.attention.wo-output', 'layers.0.feed_forward.w1-output', 'layers.0.feed_forward.w2-output', 'layers.0.feed_forward.w3-output', 'layers.0.attention_norm-output', 'layers.0.ffn_norm-output', 'layers.1.attention.wq-output', 'layers.1.attention.wk-output', 'layers.1.attention.wv-output', 'layers.1.attention.wo-output', 'layers.1.feed_forward.w1-output', 'layers.1.feed_forward.w2-output', 'layers.1.feed_forward.w3-output', 'layers.1.attention_norm-output', 'layers.1.ffn_norm-output', 'layers.2.attention.wq-output', 'layers.2.attention.wk-output', 'layers.2.attention.wv-output', 'layers.2.attention.wo-output', 'layers.2.feed_forward.w1-output', 'layers.2.feed_forward.w2-output', 'layers.2.feed_forward.w3-output', 'layers.2.attention_norm-output', 'layers.2.ffn_norm-output', 'layers.3.attention.wq-output', 'layers.3.attention.wk-output', 'layers.3.attention.wv-output', 'layers.3.attention.wo-output', 'layers.3.feed_forward.w1-output', 'layers.3.feed_forward.w2-output', 'layers.3.feed_forward.w3-output', 'layers.3.attention_norm-output', 'layers.3.ffn_norm-output', 'layers.4.attention.wq-output', 'layers.4.attention.wk-output', 'layers.4.attention.wv-output', 'layers.4.attention.wo-output', 'layers.4.feed_forward.w1-output', 'layers.4.feed_forward.w2-output', 'layers.4.feed_forward.w3-output', 'layers.4.attention_norm-output', 'layers.4.ffn_norm-output', 'layers.5.attention.wq-output', 'layers.5.attention.wk-output', 'layers.5.attention.wv-output', 'layers.5.attention.wo-output', 'layers.5.feed_forward.w1-output', 'layers.5.feed_forward.w2-output', 'layers.5.feed_forward.w3-output', 'layers.5.attention_norm-output', 'layers.5.ffn_norm-output', 'layers.6.attention.wq-output', 'layers.6.attention.wk-output', 'layers.6.attention.wv-output', 'layers.6.attention.wo-output', 'layers.6.feed_forward.w1-output', 'layers.6.feed_forward.w2-output', 'layers.6.feed_forward.w3-output', 'layers.6.attention_norm-output', 'layers.6.ffn_norm-output', 'layers.7.attention.wq-output', 'layers.7.attention.wk-output', 'layers.7.attention.wv-output', 'layers.7.attention.wo-output', 'layers.7.feed_forward.w1-output', 'layers.7.feed_forward.w2-output', 'layers.7.feed_forward.w3-output', 'layers.7.attention_norm-output', 'layers.7.ffn_norm-output', 'layers.8.attention.wq-output', 'layers.8.attention.wk-output', 'layers.8.attention.wv-output', 'layers.8.attention.wo-output', 'layers.8.feed_forward.w1-output', 'layers.8.feed_forward.w2-output', 'layers.8.feed_forward.w3-output', 'layers.8.attention_norm-output', 'layers.8.ffn_norm-output', 'layers.9.attention.wq-output', 'layers.9.attention.wk-output', 'layers.9.attention.wv-output', 'layers.9.attention.wo-output', 'layers.9.feed_forward.w1-output', 'layers.9.feed_forward.w2-output', 'layers.9.feed_forward.w3-output', 'layers.9.attention_norm-output', 'layers.9.ffn_norm-output', 'layers.10.attention.wq-output', 'layers.10.attention.wk-output', 'layers.10.attention.wv-output', 'layers.10.attention.wo-output', 'layers.10.feed_forward.w1-output', 'layers.10.feed_forward.w2-output', 'layers.10.feed_forward.w3-output', 'layers.10.attention_norm-output', 'layers.10.ffn_norm-output', 'layers.11.attention.wq-output', 'layers.11.attention.wk-output', 'layers.11.attention.wv-output', 'layers.11.attention.wo-output', 'layers.11.feed_forward.w1-output', 'layers.11.feed_forward.w2-output', 'layers.11.feed_forward.w3-output', 'layers.11.attention_norm-output', 'layers.11.ffn_norm-output', 'layers.12.attention.wq-output', 'layers.12.attention.wk-output', 'layers.12.attention.wv-output', 'layers.12.attention.wo-output', 'layers.12.feed_forward.w1-output', 'layers.12.feed_forward.w2-output', 'layers.12.feed_forward.w3-output', 'layers.12.attention_norm-output', 'layers.12.ffn_norm-output', 'layers.13.attention.wq-output', 'layers.13.attention.wk-output', 'layers.13.attention.wv-output', 'layers.13.attention.wo-output', 'layers.13.feed_forward.w1-output', 'layers.13.feed_forward.w2-output', 'layers.13.feed_forward.w3-output', 'layers.13.attention_norm-output', 'layers.13.ffn_norm-output', 'layers.14.attention.wq-output', 'layers.14.attention.wk-output', 'layers.14.attention.wv-output', 'layers.14.attention.wo-output', 'layers.14.feed_forward.w1-output', 'layers.14.feed_forward.w2-output', 'layers.14.feed_forward.w3-output', 'layers.14.attention_norm-output', 'layers.14.ffn_norm-output', 'layers.15.attention.wq-output', 'layers.15.attention.wk-output', 'layers.15.attention.wv-output', 'layers.15.attention.wo-output', 'layers.15.feed_forward.w1-output', 'layers.15.feed_forward.w2-output', 'layers.15.feed_forward.w3-output', 'layers.15.attention_norm-output', 'layers.15.ffn_norm-output', 'layers.16.attention.wq-output', 'layers.16.attention.wk-output', 'layers.16.attention.wv-output', 'layers.16.attention.wo-output', 'layers.16.feed_forward.w1-output', 'layers.16.feed_forward.w2-output', 'layers.16.feed_forward.w3-output', 'layers.16.attention_norm-output', 'layers.16.ffn_norm-output', 'layers.17.attention.wq-output', 'layers.17.attention.wk-output', 'layers.17.attention.wv-output', 'layers.17.attention.wo-output', 'layers.17.feed_forward.w1-output', 'layers.17.feed_forward.w2-output', 'layers.17.feed_forward.w3-output', 'layers.17.attention_norm-output', 'layers.17.ffn_norm-output', 'layers.18.attention.wq-output', 'layers.18.attention.wk-output', 'layers.18.attention.wv-output', 'layers.18.attention.wo-output', 'layers.18.feed_forward.w1-output', 'layers.18.feed_forward.w2-output', 'layers.18.feed_forward.w3-output', 'layers.18.attention_norm-output', 'layers.18.ffn_norm-output', 'layers.19.attention.wq-output', 'layers.19.attention.wk-output', 'layers.19.attention.wv-output', 'layers.19.attention.wo-output', 'layers.19.feed_forward.w1-output', 'layers.19.feed_forward.w2-output', 'layers.19.feed_forward.w3-output', 'layers.19.attention_norm-output', 'layers.19.ffn_norm-output', 'layers.20.attention.wq-output', 'layers.20.attention.wk-output', 'layers.20.attention.wv-output', 'layers.20.attention.wo-output', 'layers.20.feed_forward.w1-output', 'layers.20.feed_forward.w2-output', 'layers.20.feed_forward.w3-output', 'layers.20.attention_norm-output', 'layers.20.ffn_norm-output', 'layers.21.attention.wq-output', 'layers.21.attention.wk-output', 'layers.21.attention.wv-output', 'layers.21.attention.wo-output', 'layers.21.feed_forward.w1-output', 'layers.21.feed_forward.w2-output', 'layers.21.feed_forward.w3-output', 'layers.21.attention_norm-output', 'layers.21.ffn_norm-output', 'layers.22.attention.wq-output', 'layers.22.attention.wk-output', 'layers.22.attention.wv-output', 'layers.22.attention.wo-output', 'layers.22.feed_forward.w1-output', 'layers.22.feed_forward.w2-output', 'layers.22.feed_forward.w3-output', 'layers.22.attention_norm-output', 'layers.22.ffn_norm-output', 'layers.23.attention.wq-output', 'layers.23.attention.wk-output', 'layers.23.attention.wv-output', 'layers.23.attention.wo-output', 'layers.23.feed_forward.w1-output', 'layers.23.feed_forward.w2-output', 'layers.23.feed_forward.w3-output', 'layers.23.attention_norm-output', 'layers.23.ffn_norm-output', 'layers.24.attention.wq-output', 'layers.24.attention.wk-output', 'layers.24.attention.wv-output', 'layers.24.attention.wo-output', 'layers.24.feed_forward.w1-output', 'layers.24.feed_forward.w2-output', 'layers.24.feed_forward.w3-output', 'layers.24.attention_norm-output', 'layers.24.ffn_norm-output', 'layers.25.attention.wq-output', 'layers.25.attention.wk-output', 'layers.25.attention.wv-output', 'layers.25.attention.wo-output', 'layers.25.feed_forward.w1-output', 'layers.25.feed_forward.w2-output', 'layers.25.feed_forward.w3-output', 'layers.25.attention_norm-output', 'layers.25.ffn_norm-output', 'layers.26.attention.wq-output', 'layers.26.attention.wk-output', 'layers.26.attention.wv-output', 'layers.26.attention.wo-output', 'layers.26.feed_forward.w1-output', 'layers.26.feed_forward.w2-output', 'layers.26.feed_forward.w3-output', 'layers.26.attention_norm-output', 'layers.26.ffn_norm-output', 'layers.27.attention.wq-output', 'layers.27.attention.wk-output', 'layers.27.attention.wv-output', 'layers.27.attention.wo-output', 'layers.27.feed_forward.w1-output', 'layers.27.feed_forward.w2-output', 'layers.27.feed_forward.w3-output', 'layers.27.attention_norm-output', 'layers.27.ffn_norm-output', 'layers.28.attention.wq-output', 'layers.28.attention.wk-output', 'layers.28.attention.wv-output', 'layers.28.attention.wo-output', 'layers.28.feed_forward.w1-output', 'layers.28.feed_forward.w2-output', 'layers.28.feed_forward.w3-output', 'layers.28.attention_norm-output', 'layers.28.ffn_norm-output', 'layers.29.attention.wq-output', 'layers.29.attention.wk-output', 'layers.29.attention.wv-output', 'layers.29.attention.wo-output', 'layers.29.feed_forward.w1-output', 'layers.29.feed_forward.w2-output', 'layers.29.feed_forward.w3-output', 'layers.29.attention_norm-output', 'layers.29.ffn_norm-output', 'layers.30.attention.wq-output', 'layers.30.attention.wk-output', 'layers.30.attention.wv-output', 'layers.30.attention.wo-output', 'layers.30.feed_forward.w1-output', 'layers.30.feed_forward.w2-output', 'layers.30.feed_forward.w3-output', 'layers.30.attention_norm-output', 'layers.30.ffn_norm-output', 'layers.31.attention.wq-output', 'layers.31.attention.wk-output', 'layers.31.attention.wv-output', 'layers.31.attention.wo-output', 'layers.31.feed_forward.w1-output', 'layers.31.feed_forward.w2-output', 'layers.31.feed_forward.w3-output', 'layers.31.attention_norm-output', 'layers.31.ffn_norm-output'])\n"
     ]
    }
   ],
   "source": [
    "print(PerChannelStatHook.stat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Hook._acts['model.layers.0.self_attn.q_rot-output'])\n",
    "print(DistributionHook._act_counts.keys())\n",
    "print(DistributionHook._act_counts['model.layers.0.self_attn.q_rot-output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deea637-93a2-4b86-a7a6-60d26324005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "os.makedirs('vis-act-counts', exist_ok=True)\n",
    "for k, v in DistributionHook._act_counts.items():\n",
    "    v = v.cpu().numpy()\n",
    "    # print(k)\n",
    "    # print(v)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.bar(np.arange(42)-25, height=v)\n",
    "    plt.title(k)\n",
    "    plt.yscale('log')\n",
    "    plt.grid()\n",
    "    # plt.show()\n",
    "    plt.savefig(f'vis-act-counts/{k}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd97ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Hook._acts, 'acts-mix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, v  = next(iter(Hook._acts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = torch.cat(v, dim=0).transpose(1,2).contiguous().view(-1, 4096)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5ae090f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "32* 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c312a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dbcbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c474f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "save_dir = 'vis-acts'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# html = open(f'{save_dir}/index.html', 'w')\n",
    "# html.write('<html>\\n<body>\\n')\n",
    "\n",
    "for k, v in Hook._acts.items():\n",
    "    print(k, len(v))\n",
    "    # cat = torch.cat(v, dim=1).squeeze()\n",
    "    if 'rot' in k:\n",
    "        cat = torch.cat(v, dim=0)\n",
    "        print(cat.shape)\n",
    "        cat = cat.transpose(1,2).contiguous()\n",
    "        print(cat.shape)\n",
    "        cat = cat.reshape(-1, 4096)\n",
    "        print(cat.shape)\n",
    "    else:\n",
    "        continue\n",
    "        cat = torch.cat(v, dim=0).contiguous().view(-1, 4096)\n",
    "    absmax = cat.abs().max().item()\n",
    "    cat = cat.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.imshow(cat, interpolation='none', cmap='bwr', vmin=-absmax, vmax=absmax)\n",
    "    plt.title(k)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(f'{save_dir}/{k}.png')\n",
    "    plt.close()\n",
    "    # html.write(f'<img src=\"{k}.png\"></img>\\n')\n",
    "\n",
    "# html.write('</body>\\n</html>')\n",
    "# html.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = \"model.layers.22.self_attn.q_proj-output\"\n",
    "k2 = \"model.layers.22.self_attn.q_rot-output\"\n",
    "\n",
    "acts = Hook._acts\n",
    "a1 = acts[k1]\n",
    "a2 = acts[k2]\n",
    "\n",
    "# cat = torch.cat(a2, dim=0)\n",
    "cat = a2[0]\n",
    "print(cat.shape)\n",
    "cat = cat.transpose(1,2)\n",
    "print(cat.shape)\n",
    "cat = cat.reshape(-1, 4096)\n",
    "\n",
    "print(a1[0].shape)\n",
    "print(a2[0].shape)\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(a1[0][0].cpu().numpy(), interpolation='none', cmap='bwr')\n",
    "# plt.imshow(a1[0][0][:,16*128:17*128].cpu().numpy(), interpolation='none', cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cat.cpu().numpy(), interpolation='none', cmap='bwr')\n",
    "# plt.imshow(a2[0][0][16].cpu().numpy(), interpolation='none', cmap='bwr')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f53027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    print(inv_freq)\n",
    "    print(1.0/inv_freq)\n",
    "\n",
    "__init__(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3144e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
